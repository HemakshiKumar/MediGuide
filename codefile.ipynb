{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating memory for the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevent libs\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS #facebookAIsimilaritysearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = \"book/\"\n",
    "loader =  DirectoryLoader(pdf, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #pages of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting text into smaller chunks\n",
    "textsplitter = RecursiveCharacterTextSplitter(chunk_size = 500, \n",
    "                                              chunk_overlap=50)\n",
    "data_chunks = textsplitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9846"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of chunks\n",
    "len(data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "database = FAISS.from_documents(data_chunks, embedding_model)\n",
    "database.save_local(\"storedata/db_faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory is created.\n",
    "\n",
    "Connecting memory with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevent libs\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "#model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                temperature=0.7,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=True,           # ⬅️ Enable samplin       # ⬅️ Controls randomness\n",
    "                top_p=0.9)\n",
    "\n",
    "llm_to_use = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "prompt = '''\n",
    "Use the information in the context to answer the question. Only give direct response and answer only if you know, else say \"I dont know\"\n",
    "Give a 2 sentence answer only\n",
    "context: {context}\n",
    "question: {question}\n",
    "answer:\n",
    "'''\n",
    "prompt_to_feed = PromptTemplate(template=prompt, input_variables=[\"context\",\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['query']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = FAISS.load_local(\"storedata/db_faiss\", embedding_model, allow_dangerous_deserialization=True)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm_to_use,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\":5}),\n",
    "    chain_type_kwargs={\"prompt\":prompt_to_feed}\n",
    ")\n",
    "qa_chain.input_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Root canal treatment is a dental procedure that is performed to save a tooth that has become infected with dental caries or has become diseased due to extensive dental caries. The procedure involves removing the infected pulp (the living tissue within the tooth) and the surrounding bacteria and debris, and shaping the root canal into a hollow space that can be filled with a filling material. The procedure is often monitored and is performed under local anesthesia.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.invoke({\"query\": \"What is root canal treatment?\"})\n",
    "\n",
    "# If response includes full prompt, clean it up\n",
    "ans = response[\"result\"].partition(\"answer:\")\n",
    "print(ans[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
